#!/usr/bin/env python3
import argparse
import copy
import os
import random
import time
from datetime import datetime
from typing import List, Optional

import numpy as np
import pandas as pd
from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import models, transforms


os.environ.setdefault("CUBLAS_WORKSPACE_CONFIG", ":4096:8")


def seed_everything(seed: int):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    try:
        torch.use_deterministic_algorithms(True, warn_only=True)
    except TypeError:
        torch.use_deterministic_algorithms(True)


def seed_worker(_worker_id):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)


def _resolve_img_path(dataset_root: str, rel_or_abs: str) -> str:
    if os.path.isabs(rel_or_abs):
        return rel_or_abs
    rel = str(rel_or_abs).lstrip("/")
    return os.path.join(dataset_root, rel)


class RedMeatMetadataDataset(Dataset):
    def __init__(
        self,
        data_root: str,
        split: str,
        image_transform=None,
        classes: Optional[List[str]] = None,
        split_col: str = "split",
        label_col: str = "label",
        path_col: str = "abs_file_path",
    ):
        self.data_root = data_root
        self.image_transform = image_transform

        meta_path = os.path.join(self.data_root, "all_images.csv")
        if not os.path.exists(meta_path):
            raise FileNotFoundError(f"Missing metadata file: {meta_path}")

        df = pd.read_csv(meta_path)
        for c in (split_col, label_col, path_col):
            if c not in df.columns:
                raise KeyError(f"Missing column '{c}' in {meta_path}. Found: {list(df.columns)}")

        if classes is None:
            classes = sorted(df[label_col].astype(str).unique().tolist())
        self.classes = list(classes)
        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}

        sdf = df[df[split_col].astype(str) == str(split)]
        if len(sdf) == 0:
            raise ValueError(f"Split '{split}' has 0 rows in {meta_path}")

        labels_raw = sdf[label_col].astype(str).tolist()
        missing = sorted(set(labels_raw) - set(self.class_to_idx.keys()))
        if missing:
            raise ValueError(f"Split '{split}' contains labels not in class list: {missing}")

        self.paths = [_resolve_img_path(self.data_root, p) for p in sdf[path_col].astype(str).tolist()]
        self.labels = np.array([self.class_to_idx[l] for l in labels_raw], dtype=np.int64)

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        path = self.paths[idx]
        label = int(self.labels[idx])
        img = Image.open(path).convert("RGB")
        if self.image_transform is not None:
            img = self.image_transform(img)
        return img, label, path


def make_model(model_name: str, num_classes: int, pretrained: bool):
    if model_name == "resnet50":
        model = models.resnet50(pretrained=pretrained)
        model.fc = nn.Linear(model.fc.in_features, num_classes)
        return model
    if model_name == "resnet18":
        model = models.resnet18(pretrained=pretrained)
        model.fc = nn.Linear(model.fc.in_features, num_classes)
        return model
    raise ValueError(f"Unsupported model_name: {model_name}")


def _class_balanced_acc(labels_np: np.ndarray, preds_np: np.ndarray, num_classes: int):
    cls_correct = np.zeros(num_classes, dtype=np.int64)
    cls_total = np.zeros(num_classes, dtype=np.int64)
    for c in range(num_classes):
        mask = labels_np == c
        if np.any(mask):
            cls_correct[c] += np.sum(preds_np[mask] == labels_np[mask])
            cls_total[c] += np.sum(mask)
    cls_acc = cls_correct / np.maximum(cls_total, 1)
    return float(np.mean(cls_acc)), cls_acc


def evaluate_test(model, test_loader, device, num_classes: int):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    total, correct, total_loss = 0, 0, 0.0
    cls_correct = np.zeros(num_classes, dtype=np.int64)
    cls_total = np.zeros(num_classes, dtype=np.int64)

    with torch.no_grad():
        for images, labels, _paths in test_loader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True).long()
            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item() * images.size(0)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += images.size(0)

            labels_np = labels.detach().cpu().numpy()
            preds_np = preds.detach().cpu().numpy()
            for c in range(num_classes):
                m = labels_np == c
                if np.any(m):
                    cls_correct[c] += np.sum(preds_np[m] == labels_np[m])
                    cls_total[c] += np.sum(m)

    avg_loss = total_loss / max(total, 1)
    acc = 100.0 * correct / max(total, 1)
    cls_acc = cls_correct / np.maximum(cls_total, 1)
    per_group = 100.0 * float(np.mean(cls_acc))
    worst_group = 100.0 * float(np.min(cls_acc))
    return avg_loss, acc, cls_acc * 100.0, per_group, worst_group


def train_model(model, dataloaders, dataset_sizes, num_classes, num_epochs, optimizer, device):
    criterion = nn.CrossEntropyLoss()
    best_wts = copy.deepcopy(model.state_dict())
    best_balanced = -1.0
    best_epoch = -1
    since = time.time()

    for epoch in range(num_epochs):
        print(f"Epoch {epoch + 1}/{num_epochs}", flush=True)

        for phase in ["train", "val"]:
            is_train = phase == "train"
            model.train() if is_train else model.eval()

            running_loss = 0.0
            running_corrects = 0
            cls_correct = np.zeros(num_classes, dtype=np.int64)
            cls_total = np.zeros(num_classes, dtype=np.int64)

            for images, labels, _paths in dataloaders[phase]:
                images = images.to(device, non_blocking=True)
                labels = labels.to(device, non_blocking=True).long()

                if is_train:
                    optimizer.zero_grad(set_to_none=True)

                with torch.set_grad_enabled(is_train):
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    preds = outputs.argmax(dim=1)
                    if is_train:
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * images.size(0)
                running_corrects += (preds == labels).sum().item()

                labels_np = labels.detach().cpu().numpy()
                preds_np = preds.detach().cpu().numpy()
                for c in range(num_classes):
                    m = labels_np == c
                    if np.any(m):
                        cls_correct[c] += np.sum(preds_np[m] == labels_np[m])
                        cls_total[c] += np.sum(m)

            epoch_loss = running_loss / max(dataset_sizes[phase], 1)
            epoch_acc = running_corrects / max(dataset_sizes[phase], 1)
            class_acc = cls_correct / np.maximum(cls_total, 1)
            balanced_acc = float(np.mean(class_acc))

            print(
                f"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Balanced Acc: {balanced_acc:.4f}",
                flush=True,
            )

            if phase == "val" and balanced_acc > best_balanced:
                best_balanced = balanced_acc
                best_epoch = epoch
                best_wts = copy.deepcopy(model.state_dict())

    elapsed = time.time() - since
    print(f"Training complete in {elapsed // 60:.0f}m {elapsed % 60:.0f}s", flush=True)
    model.load_state_dict(best_wts)
    return model, best_balanced, best_epoch


def build_dataloaders(data_path, batch_size, num_workers, generator, classes: Optional[List[str]] = None):
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
    tfm = transforms.Compose(
        [
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ]
    )

    train_ds = RedMeatMetadataDataset(data_path, "train", tfm, classes=classes)
    val_ds = RedMeatMetadataDataset(data_path, "val", tfm, classes=train_ds.classes)
    test_ds = RedMeatMetadataDataset(data_path, "test", tfm, classes=train_ds.classes)
    num_classes = len(train_ds.classes)

    train_loader = DataLoader(
        train_ds,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        worker_init_fn=seed_worker,
        generator=generator,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        worker_init_fn=seed_worker,
        generator=generator,
    )
    test_loader = DataLoader(
        test_ds,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        worker_init_fn=seed_worker,
        generator=generator,
    )

    dataloaders = {"train": train_loader, "val": val_loader}
    dataset_sizes = {"train": len(train_ds), "val": len(val_ds)}
    return dataloaders, dataset_sizes, test_loader, num_classes


def run_single(args):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    seed_everything(args.seed)
    g = torch.Generator()
    g.manual_seed(args.seed)

    class_list = None
    if args.classes:
        class_list = [c.strip() for c in str(args.classes).split(",") if c.strip()]

    dataloaders, dataset_sizes, test_loader, num_classes = build_dataloaders(
        args.data_path, args.batch_size, args.num_workers, g, classes=class_list
    )

    model = make_model(args.model, num_classes, pretrained=args.pretrained).to(device)

    base_lr = getattr(args, "base_lr", None)
    classifier_lr = getattr(args, "classifier_lr", None)
    if base_lr is None and classifier_lr is None:
        base_lr = args.lr
        classifier_lr = args.lr
    elif base_lr is None:
        base_lr = args.lr
    elif classifier_lr is None:
        classifier_lr = args.lr

    base_params = []
    fc_params = []
    for name, param in model.named_parameters():
        if "fc" in name:
            fc_params.append(param)
        else:
            base_params.append(param)

    param_groups = [
        {"params": base_params, "lr": float(base_lr)},
        {"params": fc_params, "lr": float(classifier_lr)},
    ]

    optimizer = optim.SGD(
        param_groups,
        momentum=args.momentum,
        weight_decay=args.weight_decay,
        nesterov=args.nesterov,
    )

    print(
        f"\n=== RUN: model={args.model} epochs={args.num_epochs} "
        f"base_lr={base_lr} classifier_lr={classifier_lr} "
        f"momentum={args.momentum} wd={args.weight_decay} nesterov={args.nesterov} seed={args.seed} ===",
        flush=True,
    )

    best_model, best_balanced_val, best_epoch = train_model(
        model=model,
        dataloaders=dataloaders,
        dataset_sizes=dataset_sizes,
        num_classes=num_classes,
        num_epochs=args.num_epochs,
        optimizer=optimizer,
        device=device,
    )

    print(f"\n[VAL] Best Balanced Acc: {best_balanced_val:.4f} at epoch {best_epoch}", flush=True)

    test_loss, test_acc, class_acc, per_group, worst_group = evaluate_test(best_model, test_loader, device, num_classes)
    print(f"\n[TEST] Loss: {test_loss:.4f}  Acc: {test_acc:.2f}%", flush=True)
    for i, acc in enumerate(class_acc):
        print(f"[TEST] class_{i}: {acc:.2f}%", flush=True)
    print(f"[TEST] Per Class Mean: {per_group:.2f}%  Worst Class: {worst_group:.2f}%", flush=True)

    save_checkpoints = os.environ.get("SAVE_CHECKPOINTS", "1").lower() not in ("0", "false", "no", "n")
    if save_checkpoints:
        os.makedirs(args.checkpoint_dir, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        ckpt_name = f"vanilla_redmeat_{args.model}_seed{args.seed}_{ts}.pth"
        ckpt_path = os.path.join(args.checkpoint_dir, ckpt_name)
        torch.save(best_model.state_dict(), ckpt_path)
    else:
        ckpt_path = "NONE"
        print("[RUN DONE] Checkpoint saving disabled via SAVE_CHECKPOINTS=0", flush=True)

    print(
        f"[RUN DONE] best_balanced_val_acc={best_balanced_val:.4f} | "
        f"test_acc={test_acc:.2f}% | saved: {ckpt_path}",
        flush=True,
    )
    return best_balanced_val, test_acc, per_group, worst_group, ckpt_path


def parse_args():
    p = argparse.ArgumentParser(description="Vanilla RedMeat CNN trainer (no guidance loss).")
    p.add_argument("data_path", help="RedMeat dataset root containing all_images.csv")
    p.add_argument("--seed", type=int, default=0)
    p.add_argument("--model", choices=["resnet50", "resnet18"], default="resnet50")
    p.add_argument("--pretrained", action="store_true", default=True)
    p.add_argument("--no-pretrained", action="store_false", dest="pretrained")
    p.add_argument("--batch-size", type=int, default=96)
    p.add_argument("--num-epochs", type=int, default=150)
    p.add_argument("--lr", type=float, default=0.01)
    p.add_argument("--base-lr", type=float, default=None)
    p.add_argument("--classifier-lr", type=float, default=None)
    p.add_argument("--momentum", type=float, default=0.9)
    p.add_argument("--weight-decay", type=float, default=1e-5)
    p.add_argument("--nesterov", action="store_true", default=False)
    p.add_argument("--no-nesterov", action="store_false", dest="nesterov")
    p.add_argument("--num-workers", type=int, default=4)
    p.add_argument("--checkpoint-dir", default="Vanilla_RedMeat_Checkpoints")
    p.add_argument(
        "--classes",
        default="prime_rib,pork_chop,steak,baby_back_ribs,filet_mignon",
        help="Comma-separated class list; empty to infer from all_images.csv",
    )
    return p.parse_args()


if __name__ == "__main__":
    args = parse_args()
    run_single(args)
